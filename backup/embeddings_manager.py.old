# src/embeddings_manager.py
# ---------------------------------------------
# GPU-OPTIMIZED Embeddings Manager for BareActs PDFs
# Maximizes RTX 4060 8GB utilization
# ---------------------------------------------

import os
import json
import torch
import faiss
import numpy as np
from typing import List, Dict, Tuple
from sentence_transformers import SentenceTransformer
from .text_preprocessor import chunk_text
from .data_loader import load_bare_acts

# Paths
INDEX_PATH = 'data/vector_store/faiss_index.idx'
METADATA_PATH = 'data/vector_store/faiss_metadata.json'
EMBED_MODEL_NAME = 'all-MiniLM-L6-v2'

class EmbeddingsManager:
    def __init__(self, model_name: str = EMBED_MODEL_NAME, use_gpu: bool = True):
        """
        Initialize GPU-optimized embeddings manager.
        """
        # Setup device
        self.device = 'cuda' if (use_gpu and torch.cuda.is_available()) else 'cpu'
        print(f"{'='*60}")
        print(f"Device: {self.device.upper()}")
        
        if self.device == 'cuda':
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
            print(f"CUDA Version: {torch.version.cuda}")
        print(f"{'='*60}\n")

        # Load model with optimizations
        self.model = SentenceTransformer(model_name, device=self.device)
        
        # Enable FP16 for faster inference on GPU
        if self.device == 'cuda':
            self.model.half()  # Use FP16
            print("✓ Enabled FP16 precision for faster GPU inference")

        # Setup FAISS with GPU support
        self.use_faiss_gpu = False
        if self.device == 'cuda':
            try:
                # Try to use FAISS GPU
                self.gpu_resources = faiss.StandardGpuResources()
                self.use_faiss_gpu = True
                print("✓ FAISS-GPU initialized successfully")
            except Exception as e:
                print(f"⚠ FAISS-GPU not available: {e}")
                print("  Using FAISS-CPU (still fast for most use cases)")
        
        # Ensure storage directory exists
        os.makedirs(os.path.dirname(INDEX_PATH), exist_ok=True)

        # Load existing index if available
        if os.path.exists(INDEX_PATH):
            self._load_existing_index()
        else:
            self.index = None
            self.metas = []

    def _load_existing_index(self):
        """Load existing FAISS index and metadata."""
        try:
            # Load CPU index first
            cpu_index = faiss.read_index(INDEX_PATH)
            
            # Transfer to GPU if available
            if self.use_faiss_gpu:
                self.index = faiss.index_cpu_to_gpu(self.gpu_resources, 0, cpu_index)
                print(f"✓ Transferred index to GPU")
            else:
                self.index = cpu_index
            
            # Load metadata
            if os.path.exists(METADATA_PATH):
                with open(METADATA_PATH, 'r', encoding='utf-8') as f:
                    self.metas = json.load(f)
            else:
                self.metas = []
            
            print(f"✓ Loaded existing index with {len(self.metas)} vectors\n")
        except Exception as e:
            print(f"Error loading index: {e}")
            self.index = None
            self.metas = []

    def build_from_folder(self, data_folder: str = 'BareActs', 
                          chunk_size: int = 500, overlap: int = 100,
                          batch_size: int = 64) -> None:
        """
        Build FAISS index with GPU acceleration.

        Args:
            data_folder: Path to BareActs PDFs
            chunk_size: Characters per chunk
            overlap: Overlap between chunks
            batch_size: Chunks to embed at once (increased for GPU)
        """
        print("="*60)
        print("BUILDING FAISS INDEX (GPU-ACCELERATED)")
        print("="*60)
        
        docs = load_bare_acts(data_folder)
        dim = 384  # all-MiniLM-L6-v2 dimension

        # Initialize index
        if self.index is None:
            cpu_index = faiss.IndexFlatIP(dim)  # Cosine similarity
            
            # Transfer to GPU if available
            if self.use_faiss_gpu:
                self.index = faiss.index_cpu_to_gpu(self.gpu_resources, 0, cpu_index)
                print("✓ Created GPU-accelerated FAISS index")
            else:
                self.index = cpu_index
                print("✓ Created CPU FAISS index")

        vector_id = len(self.metas)
        total_chunks = 0

        for act_idx, (act_name, text) in enumerate(docs.items(), 1):
            print(f"\n[{act_idx}/{len(docs)}] Processing: {act_name}")
            print(f"  Text length: {len(text):,} chars")
            
            batch = []
            batch_metas = []

            # Process chunks
            for chunk in chunk_text(text, chunk_size=chunk_size, overlap=overlap):
                batch.append(chunk)
                batch_metas.append({
                    'id': vector_id,
                    'text': chunk,
                    'meta': {'act_name': act_name}
                })
                vector_id += 1

                # Process batch when full
                if len(batch) >= batch_size:
                    self._add_batch_to_index(batch, batch_metas)
                    total_chunks += len(batch)
                    print(f"  Processed {total_chunks} chunks...", end='\r')
                    batch = []
                    batch_metas = []

            # Process remaining chunks
            if batch:
                self._add_batch_to_index(batch, batch_metas)
                total_chunks += len(batch)

            # Save after each document
            self._save_index_and_metadata()
            print(f"  ✓ Completed: {total_chunks} total chunks")
            
            # Clear GPU cache
            if self.device == 'cuda':
                torch.cuda.empty_cache()

        print(f"\n{'='*60}")
        print(f"✓ INDEX BUILD COMPLETE")
        print(f"  Total vectors: {len(self.metas):,}")
        print(f"  Storage: {INDEX_PATH}")
        print(f"{'='*60}\n")

    def _add_batch_to_index(self, batch: List[str], batch_metas: List[Dict]) -> None:
        """Embed batch and add to FAISS index."""
        with torch.no_grad():  # Disable gradient computation
            embeddings = self.model.encode(
                batch,
                convert_to_numpy=True,
                show_progress_bar=False,
                batch_size=len(batch)
            )
        
        # Normalize for cosine similarity
        faiss.normalize_L2(embeddings)
        
        # Add to index
        self.index.add(embeddings)
        self.metas.extend(batch_metas)

    def _save_index_and_metadata(self) -> None:
        """Save FAISS index and metadata."""
        os.makedirs(os.path.dirname(INDEX_PATH), exist_ok=True)
        
        # Transfer index to CPU before saving
        if self.use_faiss_gpu:
            cpu_index = faiss.index_gpu_to_cpu(self.index)
            faiss.write_index(cpu_index, INDEX_PATH)
        else:
            faiss.write_index(self.index, INDEX_PATH)
        
        # Save metadata
        with open(METADATA_PATH, 'w', encoding='utf-8') as f:
            json.dump(self.metas, f, ensure_ascii=False, indent=2)

    def search(self, query: str, k: int = 5) -> Tuple[np.ndarray, np.ndarray]:
        """
        Search for top-k similar chunks.

        Args:
            query: Search query
            k: Number of results

        Returns:
            distances, indices: Arrays of shape (1, k)
        """
        if self.index is None:
            raise ValueError("Index not loaded. Build or load index first.")
        
        # Encode query
        with torch.no_grad():
            query_embedding = self.model.encode(
                [query],
                convert_to_numpy=True,
                show_progress_bar=False
            )
        
        faiss.normalize_L2(query_embedding)
        
        # Search
        distances, indices = self.index.search(query_embedding, k)
        
        return distances, indices

    def load_index(self) -> Tuple[faiss.Index, List[Dict]]:
        """Load FAISS index and metadata."""
        if not os.path.exists(INDEX_PATH) or not os.path.exists(METADATA_PATH):
            print("Index not found. Build index first.")
            return None, None

        self._load_existing_index()
        return self.index, self.metas

    def get_gpu_memory_usage(self) -> str:
        """Get current GPU memory usage."""
        if self.device == 'cuda':
            allocated = torch.cuda.memory_allocated() / 1e9
            reserved = torch.cuda.memory_reserved() / 1e9
            return f"Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB"
        return "N/A (CPU mode)"